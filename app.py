import torch
import torch.nn as nn
import torch.nn.functional as F
import gradio as gr
import os

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
BLOCK_SIZE = 64
EMBED_SIZE = 64
HEADS = 4
MODEL_PATH = 'minigpt_checkpoint.pt'
FILE_NAME = 'book.txt' # –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ª–æ–≤–∞—Ä—è

# --- –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –ú–û–î–ï–õ–ò ---
class MiniGPT(nn.Module):
    def __init__(self, vocab_size, embed_size, num_heads, block_size):
        super().__init__()
        self.block_size = block_size
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.pos_embedding = nn.Embedding(block_size, embed_size)
        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads, batch_first=True)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)
        self.fc_out = nn.Linear(embed_size, vocab_size)

    def forward(self, x):
        B, T = x.shape
        pos = torch.arange(T, device=x.device).unsqueeze(0)
        out = self.embedding(x) + self.pos_embedding(pos)
        out = self.transformer(out)
        return self.fc_out(out)

# --- –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –ò –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø ---
if os.path.exists(FILE_NAME):
    with open(FILE_NAME, 'r', encoding='utf-8') as f: text = f.read()
else:
    # Fallback —Ç–µ–∫—Å—Ç, –µ—Å–ª–∏ book.txt –Ω–µ –Ω–∞–π–¥–µ–Ω (–¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –≤—Å–µ —Ç–æ–∫–µ–Ω—ã)
    text = "<|user|>–ø—Ä–∏–≤–µ—Ç<|model|>–Ω–æ—Ä–º–∞–ª—å–Ω–æ" * 100

chars = sorted(list(set(text)))
vocab_size = len(chars)
stoi = { ch:i for i,ch in enumerate(chars) }
itos = { i:ch for i,ch in enumerate(chars) }
encode = lambda s: [stoi.get(c, 0) for c in s]
decode = lambda l: ''.join([itos[i] for i in l])

# --- –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò ---
model = MiniGPT(vocab_size, EMBED_SIZE, HEADS, BLOCK_SIZE)
if os.path.exists(MODEL_PATH):
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ CPU, —á—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è HF Spaces —Å –±–∞–∑–æ–≤—ã–º —Ç–∞—Ä–∏—Ñ–æ–º
    model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device('cpu')))
model.eval()

# --- –õ–û–ì–ò–ö–ê –ì–ï–ù–ï–†–ê–¶–ò–ò –° –¢–û–ö–ï–ù–ê–ú–ò –ò –ù–ê–°–¢–†–û–ô–ö–ê–ú–ò ---
def predict(prompt, max_length, temperature):
    if not prompt: return "–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç"
    
    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º —Ç–æ–∫–µ–Ω –ú–æ–¥–µ–ª–∏ –∫ –∑–∞–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    full_prompt = prompt.strip() + "<|model|>"
    
    context_tokens = encode(full_prompt)[-BLOCK_SIZE:]
    context = torch.tensor(context_tokens, dtype=torch.long).unsqueeze(0)
    
    generated_tokens = []
    for _ in range(max_length):
        cond = context[:, -BLOCK_SIZE:]
        with torch.no_grad():
            logits = model(cond)[:, -1, :]
            
            if temperature == 0:
                probs = F.softmax(logits, dim=-1)
                next_token = torch.argmax(probs, dim=-1).unsqueeze(0)
            else:
                probs = F.softmax(logits / temperature, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1)
            
            # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∞ –Ω–∞—á–∞–ª–æ —Ç–æ–∫–µ–Ω–∞ '<'
            if decode([next_token.item()]) == '<':
                 break

            context = torch.cat((context, next_token), dim=1)
            generated_tokens.append(next_token.item())
            
    return decode(generated_tokens)

# --- –ò–ù–¢–ï–†–§–ï–ô–° GRADIO ---
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ü§ñ MiniGPT Chat —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏")
    with gr.Row():
        with gr.Column():
            # –ü–æ–¥—Å–∫–∞–∑–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –Ω–∞—á–∏–Ω–∞—Ç—å —Å —Ç–æ–∫–µ–Ω–∞ –¥–ª—è –ª—É—á—à–µ–π —Ä–∞–±–æ—Ç—ã
            input_text = gr.Textbox(label="–í–∞—à –∑–∞–ø—Ä–æ—Å (–Ω–∞—á–∏–Ω–∞–π—Ç–µ —Å <|user|>)", placeholder="–ù–∞–ø–∏—à–∏—Ç–µ –Ω–∞—á–∞–ª–æ —Ñ—Ä–∞–∑—ã...", lines=3)
            max_len_slider = gr.Slider(minimum=10, maximum=500, value=100, step=10, label="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞")
            temp_slider = gr.Slider(minimum=0.0, maximum=2.0, value=0.7, step=0.05, label="–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (0=–¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π)")
            btn = gr.Button("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å")

        output_text = gr.Textbox(label="–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏", lines=10)
    
    btn.click(fn=predict, inputs=[input_text, max_len_slider, temp_slider], outputs=[output_text])

if __name__ == "__main__":
    demo.launch()
